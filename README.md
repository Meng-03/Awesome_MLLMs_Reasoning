# Awesome_MLLMs_Reasoning

In this repository, we will continuously update the latest papers, slides, and other valuable resources that advance MLLM reasoning, making learning more efficient for everyone!

<!-- omit in toc -->
## ðŸ“¢ Updates

- **2025.03**: We released this repo. Feel free to cite or open pull requests.

<!-- omit in toc -->
## ðŸ“š Table of Contents
- [Awesome_MLLMs_Reasoning](#awesome_mllms_reasoning)
  - [1.Generated Data Guided Post-Training](#1generated-data-guided-post-training)
  - [2.Test-time Scaling](#2test-time-scaling)
  - [3.Benchmarks](#3benchmarks)
  - [4.MLLM Reward Model Design](#4mllm-reward-model-design)
- [Open-Source Projects](#open-source-projects)

## Papers

### 1.Generated Data Guided Post-Training
[2503] [Visual-RFT: Visual Reinforcement Fine-Tuning](https://arxiv.org/abs/2503.01785) (Shanghai AI Lab)   [Code](https://github.com/Liuziyu77/Visual-RFT)![](https://img.shields.io/badge/github-2025.03-red)

[2502] [OmniAlign-V: Towards Enhanced Alignment of MLLMs with Human Preference](https://arxiv.org/pdf/2502.18411) (Shanghai AI Lab)

[2502] [Introducing Visual Perception Token into Multimodal Large Language Model](https://arxiv.org/pdf/2502.17425) (NUS)

[2502] [MM-Verify: Enhancing Multimodal Reasoning with Chain-of-Thought Verification](https://arxiv.org/pdf/2502.13383) (PKU)

[2502] [MM-RLHF: The Next Step Forward in Multimodal LLM Alignment](https://arxiv.org/pdf/2502.10391) (Kuaishou) [Code](https://github.com/Kwai-YuanQi/MM-RLHF)

[2502] [MedVLM-R1: Incentivizing Medical Reasoning Capability of Vision-Language Models via Reinforcement Learning](https://arxiv.org/pdf/2502.19634) 

[2501] [Can We Generate Images with CoT? Letâ€™s Verify and Reinforce Image Generation Step by Step](https://arxiv.org/pdf/2501.13926) 

[2501] [KIMI K1.5: SCALING REINFORCEMENT LEARNING WITH LLMS](https://arxiv.org/pdf/2501.12599) 

[2501] [URSA: Understanding and Verifying Chain-of-thought Reasoning in Multimodal Mathematics](https://arxiv.org/pdf/2501.04686) 

[2501] [LlamaV-o1: Rethinking Step-by-step Visual Reasoning in LLMs](https://arxiv.org/pdf/2501.06186) 

[2501] [Imagine while Reasoning in Space: Multimodal Visualization-of-Thought](https://arxiv.org/pdf/2501.07542) 

[2501] [Technical Report on Slow Thinking with LLMs: Visual Reasoning](https://arxiv.org/pdf/2501.01904) 

[2412] [MAmmoTH-VL: Eliciting Multimodal Reasoning with Instruction Tuning at Scale](https://arxiv.org/pdf/2412.05237) 

[2412] [Scaling Inference-Time Search with Vision Value Model for Improved Visual Comprehension](https://arxiv.org/pdf/2412.03704) 

[2412] [TACO: Learning Multi-modal Action Models with Synthetic Chains-of-Thought-and-Action](https://arxiv.org/pdf/2412.05479) 

[2412] [DIVING INTO SELF-EVOLVING TRAINING FOR MULTIMODAL REASONING](https://arxiv.org/pdf/2412.17451) 

[2412] [Progressive Multimodal Reasoning via Active Retrieval](https://arxiv.org/pdf/2412.14835) 

[2411] [Enhancing the Reasoning Ability of Multimodal Large Language Models via Mixed Preference Optimization](https://arxiv.org/pdf/2411.10442) 

[2411] [Critic-V: VLM Critics Help Catch VLM Errors in Multimodal Reasoning](https://arxiv.org/pdf/2411.18203) 

[2411] [Insight-V: Exploring Long-Chain Visual Reasoning with Multimodal Large Language Models](https://arxiv.org/pdf/2411.14432) 

[2411] [AtomThink: A Slow Thinking Framework for Multimodal Mathematical Reasoning](https://arxiv.org/pdf/2411.11930) 

[2411] [LLaVA-o1: Let Vision Language Models Reason Step-by-Step](https://arxiv.org/pdf/2411.10440v1) 

[2411] [Vision-Language Models Can Self-Improve Reasoning via Reflection](https://arxiv.org/pdf/2411.00855) 

[2403] [Visual CoT: Unleashing Chain-of-Thought Reasoning in Multi-Modal Language Models](https://arxiv.org/pdf/2403.16999) 

[2306] [Shikra: Unleashing Multimodal LLMâ€™s Referential Dialogue Magic](https://arxiv.org/pdf/2306.15195) 


### 2.Test-time Scaling
[2502] [Boosting Multimodal Reasoning with MCTS-Automated Structured Thinking](https://arxiv.org/pdf/2502.02339) 

[2502] [MLLMS KNOW WHERE TO LOOK: TRAINING-FREE PERCEPTION OF SMALL VISUAL DETAILS WITH MULTIMODAL LLMS](https://arxiv.org/pdf/2502.17422) 

[2412] [Scaling Inference-Time Search with Vision Value Model for Improved Visual Comprehension](https://arxiv.org/pdf/2412.03704) 

[2412] [Scaling Inference-Time Search with Vision Value Model for Improved Visual Comprehension](https://arxiv.org/pdf/2412.03704) 

[2409] [Visual Agents as Fast and Slow Thinkers](https://openreview.net/pdf?id=ncCuiD3KJQ) 

[2411] [Vision-Language Models Can Self-Improve Reasoning via Reflection](https://arxiv.org/pdf/2411.00855) [Code](https://github.com/njucckevin/MM-Self-Improve)

[2402] [Scaffolding Coordinates to Promote Vision-Language Coordination in Large Multi-Modal Models](https://arxiv.org/pdf/2402.12058) [Code](https://github.com/leixy20/Scaffold)

[2402] [V-STaR: Training Verifiers for Self-Taught Reasoners](https://arxiv.org/pdf/2402.06457) 

### 3.Benchmarks

[2502] [Multimodal RewardBench: Holistic Evaluation of Reward Models for Vision Language Models](https://arxiv.org/pdf/2502.14191) [Code](https://github.com/facebookresearch/multimodal_rewardbench)  

[2502] [ZeroBench: An Impossible* Visual Benchmark for Contemporary Large Multimodal Models](https://arxiv.org/pdf/2502.09696) [Code](https://zerobench.github.io/)  

[2502] [MM-IQ: Benchmarking Human-Like Abstraction and Reasoning in Multimodal Models](https://arxiv.org/pdf/2502.00698) [Code](https://acechq.github.io/MMIQ-benchmark/)  

[2502] [MME-CoT: Benchmarking Chain-of-Thought in Large Multimodal Models for Reasoning Quality, Robustness, and Efficiency](https://arxiv.org/pdf/2502.09621) [Code](https://mmecot.github.io/)

[2406] [Cambrian-1: A Fully Open, Vision-Centric Exploration of Multimodal LLMs](https://arxiv.org/pdf/2406.16860) [Code](https://github.com/cambrian-mllm/cambrian)

[2404] [BLINK: Multimodal Large Language Models Can See but Not Perceive](https://arxiv.org/pdf/2404.12390) 

[2401] [Eyes Wide Shut? Exploring the Visual Shortcomings of Multimodal LLMs](https://arxiv.org/pdf/2401.06209) 

[2312] [Vâˆ—](https://arxiv.org/pdf/2312.14135)   

### 4.MLLM Reward Model Design
[2503] [Process-based Self-Rewarding Language Models](https://arxiv.org/pdf/2503.03746) (NJU) [Code](https://github.com/Shimao-Zhang/Process-Self-Rewarding)  

[2502] [Self-Consistency of the Internal Reward Models Improves Self-Rewarding Language Models](https://arxiv.org/pdf/2502.08922) (FDU)

[2502] [Agentic Reward Modeling: Integrating Human Preferences with Verifiable Correctness Signals for Reliable Reward Systems](https://arxiv.org/pdf/2502.19328)  (THU) [Code] (https://github.com/THU-KEG/Agentic-Reward-Modeling)

[2502] [Preference Leakage: A Contamination Problem in LLM-as-a-judge](https://arxiv.org/abs/2502.01534) (Arizona State University) [Code](https://github.com/David-Li0406/Preference-Leakage)

[2502] [The Lessons of Developing Process Reward Models in Mathematical Reasoning](https://arxiv.org/abs/2501.07301) (Qwen) [Code](https://huggingface.co/Qwen/Qwen2.5-Math-PRM-7B)

## Open-Source Projects
[R1-V: Reinforcing Super Generalization Ability in Vision Language Models with Less Than $3] [Code](https://github.com/Deep-Agent/R1-V)![](https://img.shields.io/badge/github-2025.02-red)

[EasyR1: An Efficient, Scalable, Multi-Modality RL Training Framework] [Code](https://github.com/hiyouga/EasyR1)![](https://img.shields.io/badge/github-2025.02-red)  

[R1-Onevisionï¼šAn Open-Source Multimodal Large Language Model Capable of Deep Reasoning] [Code](https://github.com/Fancy-MLLM/R1-Onevision)![](https://img.shields.io/badge/github-2025.02-red)  

[LMM-R1] [Code](https://github.com/TideDra/lmm-r1)![](https://img.shields.io/badge/github-2025.02-red)  

[VLM-R1: A stable and generalizable R1-style Large Vision-Language Model] [Code](https://github.com/om-ai-lab/VLM-R1)![](https://img.shields.io/badge/github-2025.02-red)  

[Multi-modal Open R1] [Code](https://github.com/EvolvingLMMs-Lab/open-r1-multimodal)![](https://img.shields.io/badge/github-2025.02-red)  

[Video-R1: Towards Super Reasoning Ability in Video Understanding] [Code](https://github.com/tulerfeng/Video-R1)![](https://img.shields.io/badge/github-2025.02-red)  

[Open-R1-Video] [Code](https://github.com/Wang-Xiaodong1899/Open-R1-Video)![](https://img.shields.io/badge/github-2025.02-red)  

[R1-Vision: Let's first take a look at the image] [Code](https://github.com/yuyq96/R1-Vision)![](https://img.shields.io/badge/github-2025.02-red)  
